{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# funkyAD Documentation\n",
    "## *A package for automatic differentation*\n",
    "\n",
    "## Introduction\n",
    "Differentiation is used in many applications, such as finding stationary points of defined functions or minimizing objective loss functions in machine learning applications. \n",
    "But differentiating an arbitrary function &#8477;<sup>n</sup> &#8594; &#8477;<sup>m</sup> is generally not an easy task. \n",
    "When the function can be expressed as a composition of differentiable elementary functions (which in most cases is true), Automatic Differentiation (AD) can help. \n",
    "AD has become one of the most popular techniques for finding derivatives and is often preferred over symbolic differentation and numerical differentiation because of its efficiency and stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='background'></a>\n",
    "## Background\n",
    "\n",
    "*Summary*\n",
    "\n",
    "Why is AD such a popular choice for differentiation? AD takes an input function and breaks it down into a set of elementary functions. Symbolic differentiation is used to calculate the derivatives of the elementary functions. These elementary functions are then combined using common mathematical functions, such as addition or multiplication. The derivatives of the combined functions (which we will refer to as nodes) are calculated using the derivatives calculated from the earlier elementary functions. This process repeats (as more complex functions are combined) until the function's derivative has been calculated. \n",
    "\n",
    "This process can be represented using a computational trace or a computational graph. The computational trace is a table that stores the node, its elementary function, their derivatives, and the derivative values evaluated at a given input (if provided). This process can also be visually representated using a computational graph whose nodes are the same as the rows in the computational trace. Nodes are connect to show how nodes are combined and by what elementary operations. \n",
    "\n",
    "*Building a computational graph*\n",
    "\n",
    "As an example, let's visualize $f(x,y)=xy+cos(x+y)$ using a computational graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./img/computational_graph_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the function's input ($x$ and $y$) as the first nodes ($x_1$ and $x_2$) in our graph. These two inputs are combined by addition ($x+y$) and multiplication ($xy$) to create create two intermediary elementary functions represented by nodes $x_3$ and $x_4$ (notice, we can now rewrite $f(x,y) = xy+cos(x+y)$ as a function of $x_3$ and $x_4$, $f(x_3,x_4) = x_3 + cos(x_4)$). We then take the cosine of $x_4$ to get the intermediatory function $x_5$. Once again, $f(x,y)$ can now be expressed as a function of $x_3$ and $x_5$, $f(x_3,x_5)=x_3 + x_5$. Finally, $x_3$ and $x_5$ are combined by addition to create the final node $x_6$, where $f(x) = x_6$. So the derivative of $f$ is the derivative of $x_6$, which is a function of the derivatives of $x_3$ and $x_6$ which we have computed based on earlier nested functions of nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Autodifferentiation Implementation: Forward and Reverse Mode*\n",
    "\n",
    "There are two common methods for implementing AD, forward mode and backward mode (of which the popular backpropagation algorithm for neural networks is a special case). Given a function &#8477;<sup>n</sup> &#8594; &#8477;<sup>m</sup>, forward mode fixes the independent variables or n inputs to solve for m outputs; whereas backward mode fixes the dependent variables or m outputs. This means that forward mode is more efficient when m>>n and backward mode is more efficient when n>>m. As an example of how these two differ, let's return to our previous example, $f(x,y)=xy+cos(x+y)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Building a computational trace for forward mode*\n",
    "\n",
    "The evaluation trace is a table where each row is a node and every column stores information about that node. Note that we don't need to build the computational graph before the trace, but it can provide a nice means of visualizing how the function is broken down into a set of nodes. \n",
    "\n",
    "Let's walk through building an evaluation trace for our example function $f(x)$. The input nodes are the first rows in the trace. Since they are inputs, they are not broken down into further subcomponents. For this exercise, their current values are $x$ and $y$. Typically we'll be evaluating function and its derivative at a point in the $x,y$ plane. Since the function has two inputs ($x$ and $y$), the derivative will have two dimensions, one for the partial derivative of $f$ with respect to $x$ ($\\nabla x$ in our table) and another for the partial derivative of $f$ with respect to $y$ ($\\nabla y$ in our table). The values of $\\nabla x$ and $\\nabla y$ for the inputs are often referred to as the seed vector. In this example our seed vector is $[1, 1]$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./img/trace_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we add the next two intermediate nodes $x_3$ and $x_4$. Notice that the derivative values for each elementary function rely on the derivatives of the inputs to that elementary function. Take $x_3 = x_1 \\cdot x_3$, we know, by the product rule, that the derivative is is $x_1 x_3' + x_1' x_3 $ (with respect to either $x$ or $y$). But we have already calculated $x_3'$ and $x_1'$ in earlier rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./img/trace_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's complete the table. The final row $x_6=x_3+x_5$ is our function $f$. Plugging in earlier derived values for $x_6' = x_3'+x_5'$, we can find the partial derivatives wrt to $x$ and $y$. This iterative process of building up the derivative from the inputs is the intuition behind foward mode. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./img/trace_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This trace has been completed for some abstract $x$ and $y$. In most cases, we want to evaluate the function at a specific value. Using the evaluation trace, we can easily plug these values in to the table to find, for example, the function and its derivative at $(x,y)=(2,1)$:\n",
    "\n",
    "$$f(2,1) = (2)(1) + cos(2+1) = 2 + cos(3) $$\n",
    "$$f'(2,1) = [1 - sin(2+1), 2-sin(2+1)] = [1-sin(3), 2-sin(3)] $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Reverse Mode*\n",
    "\n",
    "Another option for automatic differenation is reverse mode. In forward mode, you have to calculate the derivative with respect to every input value, e.g. in our example we needed to find the partial derivative wrt $x$ and wrt $y$. If there are many inputs, this can be computationally intense. Reverse mode solves this problem by fixing the dependent variable and working backwards. Of course if there are many outputs, this advantage is lost, so it is best to use reverse mode when there are many inputs and relatively few outputs. \n",
    "\n",
    "Reverse model usings the evaluation trace and the chain role to calculate the function's derivative. Recall that we can find the derivative of $f$ with respective to $x$ by finding the partial derivatives with respect to a sub-expression $w$ as follows:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial w}\\frac{\\partial w}{\\partial x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reverse mode, these subexpression $w$ are the children nodes of our computational graph. For nodes with more than one child, the corresponding derivative according to the chain rule is therefore:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial w_1}\\frac{\\partial w_1}{\\partial x} +  \\frac{\\partial f}{\\partial w_2}\\frac{\\partial w_2}{\\partial x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try reverse mode for our example function $f(x) = xy+cos(x+y)$. We start with the final node $x_6$ and work backwards: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial f}{\\partial x_6}=1$$\n",
    "\n",
    "The final derivative is the seed, which is set to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial f}{\\partial x_5}=\\frac{\\partial f}{\\partial x_6} \\frac{\\partial x_6}{\\partial x_5} = (1) \\frac{\\partial (x_3+x_5) }{\\partial x_5} = 1\\cdot1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know $\\frac{\\partial f}{\\partial x_6}$. We can sub $x_6$ for its elementary function calculated in the trace ($x_6=x_3+x_5$) and easily take the derivative with respective to $x_5$, which is 1 in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial f}{\\partial x_4}=\\frac{\\partial f}{\\partial x_5} \\frac{\\partial x_5}{\\partial x_4}=(1) \\frac{\\partial cos(x_4)}{\\partial x_4} = -sin(x_4)$$\n",
    "\n",
    "Again, we already know $\\frac{\\partial f}{\\partial x_5}$ from the previous step. And we can plug in the elementary function fo $x_5$ using our evaluation trace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial f}{\\partial x_3}=\\frac{\\partial f}{\\partial x_6} \\frac{\\partial x_6}{\\partial x_3} = (1) \\frac{\\partial x_3+x_5}{\\partial x_3} = (1)(1)$$\n",
    "\n",
    "Similar to above, we plug in expressions previously calculated and from the trace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial f}{\\partial x_2}=\\frac{\\partial f}{\\partial x_3} \\frac{\\partial x_3}{\\partial x_2} + \\frac{\\partial f}{\\partial x_4} \\frac{\\partial x_4}{\\partial x_2} = (1)  \\frac{\\partial (x_1 x_2)}{\\partial x_2} -sin(x_1+x_2) \\frac{\\partial (x_1+x_2)}{\\partial x_2} = (1)(x_1)-sin(x_1+x_2) (1) $$\n",
    "\n",
    "Because $x_2$ has children $x_3$ and $x_4$, we need to consider both partial derivatives. We plug in the elementary functions for $x_4$ and $x_3$ using the trace. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial f}{\\partial x_1}=\\frac{\\partial f}{\\partial x_3} \\frac{\\partial x_3}{\\partial x_1} + \\frac{\\partial f}{\\partial x_4} \\frac{\\partial x_4}{\\partial x_1} = (1) \\frac{\\partial x_1 x_2}{\\partial x_1} -sin(x_1+x_2) \\frac{\\partial(x_1 + x_2)}{\\partial x_1} = (1) (x_2) - sin(x_1 + x_2) (1)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $x_1$ and $x_2$ are inputs $x$ and $y$ respectively, we can plug these values back in to find the partial derivatives wrt $x$ and $y$:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x_2} = \\frac{\\partial f}{\\partial y} = x - sin(x+y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial f}{\\partial x_1} = \\frac{\\partial f}{\\partial x} = y - sin(x+y)$$\n",
    "\n",
    "This is the same answer we got in forward mode, but we found the partials in just one pass! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use funkyAD\n",
    "\n",
    "The software funkyAD is a software package that the user will interact with using the AD class. This AD class allows the user to differentiate a specified function by wrapping it into an AD object, automatically differentiate it, and access the results. The package is intended for use by developers on personal computers, as a building block on top of which other functionality may be developed. We recommend users download the package using PyPi. A minimal installation in a fresh virtual environment might proceed as follows:\n",
    "\n",
    "    conda create -n funky python=3.7\n",
    "    source activate funky\n",
    "\n",
    "Then, to install funkyAD with PyPi use the following command:\n",
    "\n",
    "\tpip install -i https://test.pypi.org/simple/ funkyAD-funkyADers\n",
    "\n",
    "    \n",
    "You are now ready to use the package! Please see examples below.\n",
    "\n",
    "#### Importing funkyAD after installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T17:19:53.158145Z",
     "start_time": "2019-11-19T17:19:53.150157Z"
    }
   },
   "outputs": [],
   "source": [
    "# After installing the package, import base from funkyAD\n",
    "from funkyAD.base import AD, grad, Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic examples of funkyAD\n",
    "\n",
    "To use funkyAD, the user must pass a function to the AD class that he or she would like to differentiate. The gradient of this function is accessed through AD's grad function. funkyAD supports both scalar and vector functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T17:24:37.622315Z",
     "start_time": "2019-11-19T17:24:37.615862Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12.]]\n",
      "[[27.]]\n"
     ]
    }
   ],
   "source": [
    "# exponent example \n",
    "def f1(x):\n",
    "    return x ** 3\n",
    "\n",
    "ad_object = AD(f1)\n",
    "print(ad_object.grad(2))\n",
    "print(ad_object.grad(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T17:24:40.792296Z",
     "start_time": "2019-11-19T17:24:40.786224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# 2 inputs\n",
    "def f2(x, y):\n",
    "    return x + y\n",
    "\n",
    "print(AD(f2).grad(3, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T17:23:19.153292Z",
     "start_time": "2019-11-19T17:23:19.138711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]]\n",
      "[[1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Package supports variable number of inputs/outputs \n",
    "import numpy as np\n",
    "def f3(a: np.array):\n",
    "    return a.sum()\n",
    "print(AD(f3).grad(np.array([3, 5, 7])))\n",
    "print(AD(f3).grad(np.array([3, 5, 7, 9, 11])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T17:23:19.301913Z",
     "start_time": "2019-11-19T17:23:19.291946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.]\n",
      " [-1.]\n",
      " [ 5.]]\n"
     ]
    }
   ],
   "source": [
    "# Package supports multiple inputs and outputs\n",
    "def f4(x):\n",
    "    return x ** 2, -x, 5 * x\n",
    "    # equivalently can also return a np.array() object\n",
    "print(AD(f4).grad(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 1.]\n",
      " [1. 0.]\n",
      " [2. 4.]]\n"
     ]
    }
   ],
   "source": [
    "def f4b(x,y): \n",
    "    return x*y, x, x**2+y**2\n",
    "print(AD(f4b).grad(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative ways to access the gradient\n",
    "While funkyAD can be directly called as shown above, we have also created a function so that you can get the gradient directly without going through the AD object, in case this is a more intuitve way to get the gradient. The grad method creates the AD object on the function implicitely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.]]\n",
      "[[2.]]\n"
     ]
    }
   ],
   "source": [
    "# One way to get the derivative\n",
    "def f5(x):\n",
    "    return x**2\n",
    "\n",
    "print(AD(f5).grad(1))\n",
    "\n",
    "# Alternative way\n",
    "print(grad(f5)(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using elementary functions in funkyAD\n",
    "FunkyAD supports many elementary function operations such as addition, subtraction, exponents, and trigonometric functions like cosine, etc. In order to access built in functions that aren't already implicitely overloaded (e.g. addition), you need to import them from the funkyAD functions module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T17:26:19.527151Z",
     "start_time": "2019-11-19T17:26:19.521157Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.7182817]]\n"
     ]
    }
   ],
   "source": [
    "from funkyAD.functions import exp \n",
    "\n",
    "def f6(x):\n",
    "    return exp(x)\n",
    "\n",
    "print(AD(f6).grad(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding a user-defined arbitrary function\n",
    "Currently, the package only supports a subset of the possible elementary function that a user might want to use. To extend the library with a new Elementary Function, the user must provide the function and its derivative, and wrap them into a BaseFunction object as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T17:41:45.218087Z",
     "start_time": "2019-11-19T17:41:45.210951Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04517666]]\n"
     ]
    }
   ],
   "source": [
    "from funkyAD.functions import BaseFunction\n",
    "\n",
    "def sigmoid(x):\n",
    "    # Use x.v for the value of x\n",
    "    return np.exp(x.v) / (1 + np.exp(x.v))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    # Use x.v for the value, x.d for the derivative of the argument (x dot)\n",
    "    return x.d * np.exp(-x.v) / (1 + np.exp(-x.v)) ** 2\n",
    "\n",
    "sig = BaseFunction(sigmoid, sigmoid_derivative)\n",
    "\n",
    "print(grad(sig)(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T17:41:45.392441Z",
     "start_time": "2019-11-19T17:41:45.386172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07065083 0.        ]\n",
      " [1.         0.19661194]]\n"
     ]
    }
   ],
   "source": [
    "# You can also use sig as a part of a larger function as follows:\n",
    "def f7(x, y):\n",
    "    return sig(x ** 2), x + sig(y)\n",
    "print(grad(f7)(2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T17:41:45.790455Z",
     "start_time": "2019-11-19T17:41:45.782675Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Multiple input functions are also supported\n",
    "weird_add = BaseFunction(lambda x, y: 2 * x.v + y.v, lambda x, y: 2 * x.d + y.d)\n",
    "print(grad(weird_add)(3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using funky AD for Newton's Method\n",
    "\n",
    "funkyAD can be used for more than just deriving derivatives of simple functions. One use case is for root finding. Newton's Method is a root-finding algorithm that finds roots using an iteration approach: \n",
    "\n",
    "$$x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}$$\n",
    "\n",
    "This algorithm requires calculating the derivative in order to update. funkyAD can be used to find the derivative in this case. Take the function $f(x)=x^2+x$. It has two roots, one at -1 and another at 0. Using Newton's Method we can find both these roots by using different initialization points (e.g. if we always pick a positive initialization point, then the only root it will find is 0). Please see below for the code and results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T17:43:41.343969Z",
     "start_time": "2019-11-19T17:43:40.967504Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T17:43:41.355294Z",
     "start_time": "2019-11-19T17:43:41.346665Z"
    }
   },
   "outputs": [],
   "source": [
    "# create newton root method using funkyAD to find the Jacobian of the function\n",
    "def newtonroot(f, x):\n",
    "    fx = f(x)\n",
    "    dfx = AD(f).grad(x) # alternatively this can be written grad(f)(x)\n",
    "    x_next = x - fx/dfx \n",
    "    return x_next \n",
    "\n",
    "def get_root(f,x_start):\n",
    "    delta = 10\n",
    "    path = [x_start]\n",
    "    x = x_start\n",
    "    while delta > 1e-6:\n",
    "        new_x = newtonroot(f,x) \n",
    "        path.append(new_x)\n",
    "        delta = abs(x - new_x)\n",
    "        x = new_x \n",
    "    return(x, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T17:43:49.641458Z",
     "start_time": "2019-11-19T17:43:49.632435Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]] [[-1.]]\n"
     ]
    }
   ],
   "source": [
    "# root method for scalar function (e.g. y = f(x))\n",
    "def f(x):\n",
    "    return x**2+x \n",
    "\n",
    "# try one initialization\n",
    "zero1, path1 = get_root(f, 1)\n",
    "\n",
    "# try a different initialization \n",
    "zero2, path2  = get_root(f, -2)\n",
    "print(zero1, zero2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T17:43:42.373005Z",
     "start_time": "2019-11-19T17:43:42.125775Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xs = np.linspace(-2.5, 1.5, 100)\n",
    "path1_ys = [f(x) for x in path1]\n",
    "path2_ys = [f(x) for x in path2]\n",
    "\n",
    "\n",
    "plt.plot(xs, f(xs))\n",
    "plt.axhline(y=0, color='r', linestyle='-')\n",
    "plt.scatter(path1, path1_ys)\n",
    "plt.scatter(path2, path2_ys)\n",
    "plt.title(r'Root finding for $x^2 + x$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software Organization\n",
    "\n",
    "#### Directory Structure: \n",
    "```\n",
    ".\n",
    "| README.md\n",
    "| LICSENSE\n",
    "| requirements.txt \n",
    "| setup.py\n",
    "| .travis.yml\n",
    "| .gitignore \n",
    "| \n",
    "|--docs\n",
    "|  | documentation.ipynb \n",
    "|  | milestone1.md\n",
    "|  | milestone2.ipynb\n",
    "|  |-img\n",
    "|. |-benchmarks\n",
    "|  |-examples\n",
    "|  | |- AD_users_intro.ipynb \n",
    "|  | |- example1.py\n",
    "|  | |- trace.py\n",
    "|\n",
    "|--src/funkyAD\n",
    "|  | __init__.py \n",
    "|  | base.py\n",
    "|  | functions.py \n",
    "|  | helpers.py\n",
    "|\n",
    "|--tests\n",
    "|  | test_base.py\n",
    "|  | test_functions.py\n",
    "|  | test_helpers.py\n",
    "```\n",
    "\n",
    "#### Modules (functionality)\n",
    "\n",
    "The funkyAD package stored within the \\src directory contains 3 modules: `base.py`, `functions.py`, and `helpers.py`. \n",
    "\n",
    "The **base module** defines the AD class and the Node class. The AD class takes a function as an input and initiates forward or reverse mode to find the gradient of the function at a given value. The input function is broken down into nodes (defined by the Node class) which store the value (Node.v) and derivative (Node.d) for each input and combined nodes, leveraging the properties of dual numbers. \n",
    "\n",
    "The **functions module** defines the values and derivatives of elementary functions (e.g. sin, exponents, addition, and multiplication). Since we are working with Node objects, the Base Function class converts the inputs into nodes and then defines the corresponding value and derivative for the output node. As an example, consider if we were trying to define the elementary function multiplication with two inputs, $x^2$ and $y$. We return an output node whose values and derivatives defined based on the symbolic differentiation for multiplication, so in our example, the output node is Node($x^2+y, 2xy + x^2)$.\n",
    "\n",
    "The **helpers module** includes helper functions such as figuring out the number of inputs in a function and turning all inputs into nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test suite\n",
    "Our test suite consists of doctests, PyTest, TravisCI, and CodeCov. Pytest is used to run automated tests. TravisCI is used to run the tests when a pull request is made to merge changes into master, so that we only publish code bases that successfully build. CodeCov is used to track code coverage (percentage of functions covered by tests) with each build and through the code base history.\n",
    "\n",
    "Importantly, testing is done against an installed version of the code base. This is how users will interact with the package, and for development purposes avoids problems of Python being confused between imports of `funkyAD` referring to an installed package or a local library. With the application code moved within a `src` directory and tests run against an installed version, there will be no conflicts.\n",
    "\n",
    "##### Continuous integration\n",
    "The test suite automatically runs with pytest on Travis CI whenever an edit to the production code is initiated. The package contains the `YAML` manifest for Travis, which specifies what dependencies to install and does so automatically, before running the tests every time a pull request (or push) is made to update the code in the master branch.\n",
    "\n",
    "We have three sets of unit tests: `test_base.py`, `test_functions.py`, and `test_helpers.py`, for each module respectively. They check for basic functionality of classes and functions, test that exceptions are handled properly, and verify that our functions work in various edge cases and with different input arrays.  \n",
    "\n",
    "##### A word on package structure\n",
    "Rather than being placed among the application code, the tests are in a `/tests` folder in the root directory. Every time Travis runs, it sets up a virtual environment with Python, installs our package dependencies, then installs the package from the local source code using the `setup.py` file (not the version published in PyPi, as that does not contain the latest changes)\n",
    "\n",
    "This design choice boils down to whether tests should be placed in and run against source code in the same directory, or placed outside the code base and run against an installed version of the code base (i.e. the package). The former is easier to implement and to use for ad hoc experimentation, but the latter separates the tests from the package itself, which is more robust and has several advantages:\n",
    "1.\tSome tests require additional dependencies beyond those of the package, starting with `pytest` itself. The user shouldn’t need to install these as well in order to use the package, which doesn’t actually require them.\n",
    "2.\tMost users will not run the tests, assuming rather that the developers tested it properly before publishing it. As such, tests are part of the distribution, but not directly part of the package.\n",
    "3.\tIt avoids the issue where there is both an installed version of the package and the local source code, in which case imports can be unreliable in which version they import.\n",
    "4.\tDevelopers can more safely edit tests without accidently editing source code.\n",
    "5.\tIt forces developers to install the distribution in order to run tests, meaning you both test that it is packaged correctly and avoid tests failing for the wrong reasons.\n",
    "In practical terms, separating tests from the package means nesting the package one level deeper, we following a common convention and wrap it in a src directory, with tests in a separate tests directory in the distribution.\n",
    "\n",
    "##### Running the tests\n",
    "As with using the package, we recommend running tests in a virtual environment. First, set up a virtual environment, in this example called funky:\n",
    "\n",
    "    conda create -n funky python=3.7 anaconda\n",
    "    conda activate funky\n",
    "\n",
    "Then you can either install the funkyAD package via PyPi:\n",
    "\n",
    "    pip install -i https://test.pypi.org/simple/ funkyAD-funkyADers\n",
    "\n",
    "Or by installing the local source code on your machine from inside the cloned repo:\n",
    "\n",
    "    pip install -r requirements.txt\n",
    "    pip install -e .\n",
    "\n",
    "The `-e` flag makes the source code editable, so that you can make changes to the code base and re-run tests without having to reinstall the package from the updated local source code every time you make a change.\n",
    "\n",
    "So far the only packages installed in your new virtual environment are those listed in the requirements file. This does not include the packages necessary to run the unit tests, as most users will not be needing that. In order to run the tests, you will need to install pytest and, optionally, codecov: \n",
    "    \n",
    "    pip install pytest pytest-cov\n",
    "    pip install codecov\n",
    "\n",
    "Now you can run all tests in the test suite via:\n",
    "\n",
    "    pytest --cov funkyAD\n",
    "\n",
    "Where the `--cov` flag is optional depending on whether you want to see reports on code coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Software Packaging & Distribution \n",
    "\n",
    "The funkyAD package will be distributed with PyPI. Consequently, users will be able to install the package using the ubiquitous pip package manager. It will follow the guidelines and instructions in the official Python documentation. \n",
    "\n",
    "\tpip install -i https://test.pypi.org/simple/ funkyAD-funkyADers\n",
    "\n",
    "We recommend using a virtual environment, which can be started with the following command before downloading the package: \n",
    "\n",
    "\tconda create -n env_name python=3.7 anaconda\n",
    "\tsource activate env_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "funkyAD has three modules (`base.py`, `functions.py`, and `helpers.py`), which we will describe in more detail here.\n",
    "\n",
    "##### `base.py`\n",
    "\n",
    "*AD Class* - The AD class takes a function as input and has four parameters for storing the function (`self.f`), the seed (`self.seed`), the input dimension (`self.n`), the output dimension (`self.m`), and the trace (`self.trace`). The input function is initialized automatically but the remaining parameters are not filled in until the user either gralls AD's grad function to run forward mode, which calls AD's private function `\\_forward` to find the derivative of the function at the values specified in the grad function using forward mode (e.g. AD(f).grad(2) finds the gradient of function f at 2). `\\_forward` first determines the number of inputs and outputs (self.n and self.m, respectively) in the given function. We handle non-scalar functions by storing everything in ndarrays. After determining the dimension of the inputs and outputs,`\\_forward` sets the seed if the user hasn't done that already, checking to make sure the seed is of the right dimension and input type. Then, `\\_forward` calls the helper function `nodify` to convert all values to Nodes (described below), which are returned to the grad function and the derivatives returned to the user as an ndarray using the helper function `\\_unpack`. \n",
    "\n",
    "*Node Class* - The Node class defines a node, which has two main parameters: a value and a derivative. Nodes are connected and can be added or multiplied together to form new nodes, via the dunder methods`\\_\\_add\\_\\_` etc. These dunder methods are overloaded with elementary functions defined in the functions.py file. We check to make sure all values and derivatives are numeric, i.e. it does not support strings. While the behavior for the common set of operatures ($+, \\times, sin,$ etc.) is known, we make our own design choices for comparision operators. For equality (=) and inequality (!=) comparisons of nodes, we check that both the value and the derivative ar the same or that either differ, respectively. For comparison operators ($<, <=, >, >=$), we only use the value of the node and ignore the derivatives. \n",
    "\n",
    "\n",
    "*Grad function* -  As syntatic sugar, we also define a function `grad` which calls the AD class implicitely to return the gradient.  \n",
    "\n",
    "<font color='red'> This is the forward mode component of our package, please refer to the [Extension](#extension) section for details on how reverse mode is implemented.</font> \n",
    "\n",
    "\n",
    "##### `functions.py` \n",
    "\n",
    "The Base Function class converts inputs into nodes and then defines the corresponding value and derivative for the output node. As an example, consider if we were trying to define the elementary function multiplication with two inputs, $x^2$ and $y$. We return an output node whose values and derivatives defined based on the symbolic differentiation for multiplication, so in our example, the output node is Node($x^2+y, 2xy + x^2)$.\n",
    "\n",
    "funkyAD currently supports the elementary functions: multiplication, division, power, sqrt, positive, negativ, absolut value, invert, round, floor, ceiling, trunc, exponentials, sin, cos, tan, arcsin, arccos, arctan, sinh, cosh, tanh, and sigmoid. The user can use elementary functions by importing them with the following line of code: \n",
    "\n",
    "    from funkyAD.functions import exp \n",
    "\n",
    "We also allow the user to add their own elementary function to the list if we do not include the elementary function they need in the initial library list. An appropriate exception is raised if the user tries to utilize a function that is not defined as an instance of the ElementaryFunction class.\n",
    "\n",
    "##### `helpers.py` \n",
    "\n",
    "Helpers defines important functions called by the AD class to perform autodifferentation: \n",
    "\n",
    "`count_recursive` takes args as input and counts the number of arguments. It is used to determine the number of inputs or outputs for a given function. \n",
    "\n",
    "`count_recursive_recursion_part` is a helper for `count_recursion` function that performs the recursion.\n",
    "\n",
    "`unpack` takes nested arrays or lists and creates a depth 1 list. This is useful for iterating  through outputs to return the derivative stored at each output node. \n",
    "\n",
    "`unpack_recursion_part` is a helper for *unpack* that performs the recursion.  \n",
    "\n",
    "`nodify` turns all inputs into nodes, storing both the value and derivative. To do this, it first checks whether each item in the args list is a ndarray, list, or other object. Depending on the type of object, it recursively sets each element in the ndarray, list, or other object as a Node with a value and a derivative and appends this to a list using the augment function defined within nodify. \n",
    "\n",
    "`recursive_append` is a helper for building the trace by storing the nodes in a list.\n",
    "\n",
    "#### External dependencies\n",
    "\n",
    "Our software package relies on numpy, which is specified in the \"requirements.txt\" file in the root directory. The testing suite reiles on pytest, pytest-cov, and codecov. Testing dependencies are specified in the \".travis.yml\" file also in the root directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='extension'></a>\n",
    "## Extension: Reverse mode \n",
    "\n",
    "As an additional feature, we have also implemented reverse mode. Similar to forward mode, reverse mode is called using the AD class defined in `base.py`. Users can specify that they'd like to use reverse mode rather than forwad mode by changing the `mode` parameter of the AD class, through the `set_mode` function, to 'reverse'. The current default is 'forward'. After changing the default, the user calls the `grad` function as usual to find the derivative of the function.\n",
    "\n",
    "The `grad` function now calls two different methods depending on whether we are in forward or reverse mode. Forward mode calls the `\\_forward` method in AD (explained above). Reverse mode calls the `\\_reverse` method. The first step in implementing reverse mode is to store the trace (self.trace), so the `\\_reverse` method startings by calling the `\\_buildtrace` to create the trace. The hierarchy of the trace is mantained by each node keeping track of its parents. This required extending the Node class to include a `parents` parents parameter which points to the parent nodes of each node, if they exist. The `recursive_append` method in the `helpers.py` builds the list recursively. Once the trace has been built (in `\\_buildtrace`), it returns thte trace to the `\\_reverse` method. The `\\_reverse` method then loops through the trace, calculating the partial derivatives along the way using the chain rule (see [Background](#background) ) for details). Finally we return the gradient.\n",
    "\n",
    "### How to use revese mode in funkyAD\n",
    "\n",
    "funkyAD is defaulted to use 'forward' mode, so if you would like to instead use 'reverse' mode, you must set the mode to reverse mode with the *set_mode* function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x,y):\n",
    "    return x**2 + y\n",
    "\n",
    "adobj = AD(f1)\n",
    "adobj.set_mode('reverse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting reverse mode, you can use the same functionality as forward mode to find your gradients. Below, we provide some examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adobj.grad(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [2., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f2(x,y):\n",
    "    return [x+y, x**2]\n",
    "\n",
    "adobj = AD(f2)\n",
    "adobj.set_mode('reverse')\n",
    "\n",
    "adobj.grad(1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set it back to forward mode the same way, with `adobj.set_mode('forward')`, but new AD object will automatically call forward mode so the user must specify reverse mode each time a new AD object is called. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f3(x,y):\n",
    "    return x**2+x\n",
    "\n",
    "adobj = AD(f3)\n",
    "adobj.set_mode('forward')\n",
    "adobj.grad(1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The benefits of reverse mode\n",
    "\n",
    "Reverse mode can be more efficient than forward mode in situations when there are far more inputs than outputs. As an example consider a function that sums all of its inputs, so $f(x_1, x_2, ..., x_n) = x_1 + x_2 + ... + x_n$. As $n$ grows, reverse mode will find the gradient faster than reverse mode. In the figure below, we see that up until about 2000 inputs, the two modes perform similarily, but after this point, forward mode time begins to grow at a much faster rate than reverse mode. At 8000 inputs, reverse mode is about twice as fast, and at 10,000 it is more than 3 times as fast. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./img/fwd_rev_increasing_n.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
