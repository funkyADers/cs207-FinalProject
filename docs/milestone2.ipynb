{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2\n",
    "\n",
    "## Introduction\n",
    "Differentiation is used in many applications, such as finding stationary points of defined functions or minimizing objective loss functions in machine learning applications. \n",
    "But differentiating an arbitrary function &#8477;<sup>n</sup> &#8594; &#8477;<sup>m</sup> is generally not an easy task. \n",
    "When the function can be expressed as a composition of differentiable elementary functions (which in most cases is true), Automatic Differentiation (AD) can help. \n",
    "AD has become one of the most popular techniques for finding derivatives and is often preferred over symbolic differentation and numerical differentiation because of its efficiency and stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "## Background\n",
    "So how does AD do it? AD takes an input functions and breaks it down into a set of elementary functions combined using common mathematical functions, such as addition or multiplication. Then, leveraging the magic of the chain rule, the function's derivatives are calculated using the partial derivatives w.r.t. the inputs. Basically you compute an evaluation trace (which can be stored in either a table or a graph), where at each intermediate step of the computation you store the current value(s) of the intermediate variables and their derivatives.\n",
    "\n",
    "As an example, if you want to compute the derivative of *sin(tan(xy) + cos(x + y))* you can first compute the derivatives w.r.t *x* and *y* of *tan(xy)* and *cos(x + y)*, then add those together, and then get the derivative of the entire function using the chain rule. This is not done symbolically, but rather numerically, for every given input.\n",
    "\n",
    "There are two common methods for implementing AD, forward AD and backward AD (of which the popular backpropagation algorithm for neural networks is a special case), which differ in efficiency based on the dimension of input/outputs. \n",
    "\n",
    "#### Updates to background from milestone1 comments:\n",
    "Some key points are missing.  For example, what does forward mode actually compute?  What is the computational trace / graph?  Why / when is the forward mode more efficient than the reverse mode?\n",
    "\n",
    "This statement is misleading:  \"This is not done symbolically, but rather numerically, for every given input.\"  Actually, AD uses symbolic derivatives of elementary functions and evaluates them at specific values.  This is in contrast to symbolic differentiation which uses symbolic differentiation of entire expressions and numerical differentiation which does not use any symbolic derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use funkyAD\n",
    "\n",
    "The software funkyAD is a software package that the user will interact with using the AD class. This AD class allows the user to differentiate a specified function by wrapping it into an AD object, automatically differentiate it, access the results and inspect the intermediate steps (if desired).\n",
    "\n",
    "The package is intended for use by developers on personal computers, as a building block on top of which other functionality may be developed.\n",
    "\n",
    "Please refer to examples in the \\examples folder.\n",
    "\n",
    "Pseudocode on how to interact with funkyAD shown below: \n",
    "```\n",
    "from funkyAD import AD\n",
    "\n",
    "# define a function to differentiate \n",
    "def f(x):\n",
    "   return 2*x+3\n",
    "   \n",
    "# Option 1: directly get the gradient or output \n",
    "AD(f).gradient(5) # outputs 10\n",
    "\n",
    "# Option 2: create an AD object, and interact with it as you wish, \n",
    "#  e.g. obtain the gradient at a specified value\n",
    "#  e.g. obtain the evaluation trace\n",
    "obj = AD(f)\n",
    "obj.gradient(5) # outputs 10\n",
    "obj.eval_trace() # outputs the evaluation table\n",
    "\n",
    "# Option 3: syntactic sugar for simpler syntax\n",
    "from funkyAD import gradient\n",
    "gradient(f)(5) # outputs 10\n",
    "# funkyAD accepts functions with multiple inputs\n",
    "def f2(x,y):\n",
    "    return 2x+y\n",
    "def f3(a -> np.array):\n",
    "\treturn a[1] + a[2]\n",
    "\n",
    "obj2 = AD(f2).gradient(4, 5)\n",
    "obj3 = AD(f3).gradient(np.array([3, 4, 5]))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software Organization\n",
    "\n",
    "#### Directory Structure: \n",
    "```\n",
    ".\n",
    "| README.md\n",
    "| LICSENSE\n",
    "| setup.py\n",
    "| .gitignore\n",
    "| \n",
    "|--docs\n",
    "|  |-base\n",
    "|  |-utils\n",
    "|  | milestone1.md\n",
    "|  | milestone2.ipynb\n",
    "|\n",
    "|--funkyAD\n",
    "|  |-tests\n",
    "|  |-base\n",
    "|  |-utils\n",
    "|\n",
    "|--examples\n",
    "|  | tutorials\n",
    "|\n",
    "|--benchmarks\n",
    "```\n",
    "\n",
    "##### Feedback from milestone1 comments\n",
    "Directory structure could be a bit more detailed re: what files each folder contains.\n",
    "\n",
    "#### Modules (functionality)\n",
    "\n",
    "The basic package will include a module for forward AD, which takes a function as an argument and\n",
    "can return any of the following: the derivative, the derivative evaluated at a given value, the trace, etc. We also hope to implement a backward AD module, which takes the same input and returns the same output as the forward AD module, but uses backpropagation to calculate the return values and is more efficient for single-output functions.\n",
    "\n",
    "##### Feedback from milestone1 comments\n",
    "Module explanation is very high-level; more detail about each module's functionality would be nice. \n",
    "\n",
    "#### Test suite\n",
    "Our test suite will live within the funkyAD directory. We plan to use TravisCI, CodeCov, doctests and unittests for testing. \n",
    "We plan to use unittests for most of our testing (dottests only occasionally). We aim for as much code coverage as possible, and specifically target Exception raising and handling of edge cases.\n",
    "\n",
    "We plan to write extensive and detailed docstrings for all functions that will be accessible to the user, and replicate those in a nicer format in the corresponding docs folder (e.g. the file /funkyAD/AD.py will have its docs in /funkyAD/AD.html). A nice collection of examples for installation and usage will hopefully help the user in getting a working knowledge of the library quickly.\n",
    "\n",
    "#### Distribution\n",
    "The funkyAD package will be distributed with PyPI. Consequently, users will be able to install the package using the ubiquitous pip package manager. It will follow the guidelines and instructions in the official Python documentation.\n",
    "\n",
    "#### Software packaging \n",
    "According to common practice, we will include \\_\\_init\\_\\_.py files and setup.py so that automated tools can install and set up the library properly. We will choose transparent file names so that import statements are intuitive to the end user.\n",
    "\n",
    "It will be packaged as a Wheel for fast and easy installation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "#### Implementation (4/5)\n",
    "1. You needed a more in-depth explanation of how you will deal with vector-valued functions.\n",
    "2. How will you be storing \"eval_trace\" and \"graph\" in the AD class?\n",
    "\n",
    "In funkyAD we define 3 main classes: AD, Node, and ElementaryFunction. AD is the class that the user will interact with. It takes in an arbitrary function from the user and calls the necessary functions and classes in order to calculate the gradient. It will have transparently named methods and syntactic sugar when appropriate. The user should not know how anything else in the library works for simple usage.\n",
    "\n",
    "The Node class is essentially a row in our trace table, it has subclasses for input nodes (InputNode) and output nodes (OutputNodes). Nodes are connected and can be added or multiplied together to form new nodes, via the dunder methods\\_\\_add\\_\\_ etc, which allows us to build up the trace table. Dual numbers are essentially encoded as the pair (Node.val, Node.grad_val) in our implementation, and additions/operation are performed when calling ElementaryFunction(Node1, Node2). A Node is, essentially, an extension of the Box class in HW4. If the user has defined a function f, we will call f(InputNode(input)) and the successive operation performed on that node will allow us to recover the evaluation trace.\n",
    "\n",
    "The ElementaryFunction Class defines the functions and derivatives of elementary functions passed in by the user, such as sin, log, etc. We also allow the user to add their own elementary function to the list if we do not include the elementary function they need in the initial library list. An appropriate exception is raised if the user tries to utilize a function that is not defined as an instance of the ElementaryFunction class.\n",
    "\n",
    "```\n",
    "# External dependencies\n",
    "import numpy\n",
    "import doctest\n",
    "import unitest\n",
    "import pytest\n",
    "\n",
    "# the AD class instigates the differentation process and stores all output values\n",
    "class AD():\n",
    "\n",
    "Methods:\n",
    " \n",
    "def __init__(): initialize AD object \n",
    "def _buildgraph(): build the evaluation graph\n",
    "def gradient(forward): run forward AD, allows for option for backward AD if implemented\n",
    "def _createnodes(): create notes for the evaluation trace\n",
    "def set_seed(): set the seed (by default is the identity matrix)\n",
    "def get_seed(): return the seed\n",
    "def eval_trace: return evaluation trace  \n",
    "def print_graph(): return the evaluation graph   \n",
    "\n",
    "Attributes: \n",
    "\n",
    "nodeList: list of nodes\n",
    "inputNodeList: the list of inputs\n",
    "outputNodelist: the list of outputs\n",
    "eval_trace: the evaluation trace   \n",
    "graph: the evaluation graph \n",
    "history: history of calls memoized for increased performance\n",
    "(dunder methods e.g. __str__ redefined)\n",
    "\n",
    "# Node classes and subclasses \n",
    "class Node(): the node class stores relevant information for each node in the graph\n",
    "class InputNode(Node): a subclass of the Node class\n",
    "class OutputNode(Node): a subclass of the Node class\n",
    "\n",
    "Methods:\n",
    "__add__  (__radd__): add two nodes\n",
    "__mult__ (__rmult__): multiple two nodes\n",
    "(other dudner methods redefined as appropriate)\n",
    "\n",
    "Attributes:   \n",
    "val: the value of the node   \n",
    "gradient_val: the gradient of the node\n",
    "parents: list of parent nodes\n",
    "children: list of children nodes\n",
    "el: elementary function that the node computes (an ElementaryFunction object)\n",
    "SHOULD THIS BE STORED AS A DUAL NUMBER?\n",
    "\n",
    "# The Elementary Function stores elementary functions and their derivatives\n",
    "class ElementaryFunction(): \n",
    "\n",
    "Methods:\n",
    "__call__(): returns the output of the function\n",
    "derivative(): returns the derivative for a given input\n",
    "\n",
    "Attributes: \n",
    "ninputs: number of inputs  \n",
    "noutputs: number of outputs \n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "**OUTSTANDING**\n",
    "At this point we are still considering how to deal with arbitrary-length arrays. One option might be to subclass np.array so it includes the functionalities we need. Another option is to have the user declare the length of the array they are passing to the function so we can create an appropriate amount of InputNodes and OutputNodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
