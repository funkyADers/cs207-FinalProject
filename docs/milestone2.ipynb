{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2\n",
    "\n",
    "## Introduction\n",
    "Differentiation is used in many applications, such as finding stationary points of defined functions or minimizing objective loss functions in machine learning applications. \n",
    "But differentiating an arbitrary function &#8477;<sup>n</sup> &#8594; &#8477;<sup>m</sup> is generally not an easy task. \n",
    "When the function can be expressed as a composition of differentiable elementary functions (which in most cases is true), Automatic Differentiation (AD) can help. \n",
    "AD has become one of the most popular techniques for finding derivatives and is often preferred over symbolic differentation and numerical differentiation because of its efficiency and stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "## Background\n",
    "\n",
    "How does AD do it? AD takes an input function and breaks it down into a set of elementary functions. It uses symbolic differentiation of the elementary functions to calculate their derivatives at specific values. These elementary functions are then combined using common mathematical functions, such as addition or multiplication. The derivatives of the combined functions (which we will refer to as nodes) can be found using the chain rule and the derivatives calculated from the earlier elementary functions. This process repeats (as more complex functions are combined) until the full functions derivative has been calculated. As an example, if you want to compute the derivative of *sin(tan(xy) + cos(x + y))* you can first compute the derivatives w.r.t *x* and *y* of *tan(xy)* and *cos(x + y)*, then add those together, and then get the derivative of the entire function using the chain rule. \n",
    "\n",
    "There are two common methods for implementing AD, forward mode and backward mode (of which the popular backpropagation algorithm for neural networks is a special case). Given a function &#8477;<sup>n</sup> &#8594; &#8477;<sup>m</sup>, forward mode fixes the independent variables or input (n) to solve for m outputs; whereas backward mode fixes the dependent variables or outputs (m). This means that forward mode is more efficient when m>>n and backward mode is more efficient when n>>m. That's why backward mode is more common in \"big data\" problems where there are often far more features than data. \n",
    "\n",
    "The AD process can be represented using either the computational trace or the computation graph. The computational trace is a table that stores the elementary and intermediary functions, their derivatives, and the derivative values evaluated at an input (if provided). This process can also be visually representated using a computational graph, where nodes are rows in the computational trace and arrows show how nodes are combined and by what elementary operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use funkyAD\n",
    "\n",
    "The software funkyAD is a software package that the user will interact with using the AD class. This AD class allows the user to differentiate a specified function by wrapping it into an AD object, automatically differentiate it, access the results and inspect the intermediate steps (if desired). The package is intended for use by developers on personal computers, as a building block on top of which other functionality may be developed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ea58fd8f4a52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import base from funkyAD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfunkyAD\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# basic functionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0madobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/funkyAD/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;31m# Import statement has to be at the bottom for some reason\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maddition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplication\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpower\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhelpers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcount_recursive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodify\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munpack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'functions'"
     ]
    }
   ],
   "source": [
    "# import base from funkyAD\n",
    "from funkyAD import base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic examples of funkyAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-235861dff693>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AD' is not defined"
     ]
    }
   ],
   "source": [
    "# show basic functionality of funkyAD\n",
    "def f(x):\n",
    "    return x**3\n",
    "\n",
    "print(AD(f).grad(2))\n",
    "print(AD(f).grad(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x,y):\n",
    "    return x+y\n",
    "\n",
    "print(AD(f).grad(3,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Using funky AD for Newton's Method\n",
    "\n",
    "$$x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate an automatic differentiation object to be used in the forward mode.\n",
    "def newtonroot(f, x):\n",
    "    # find f(x) and f'(x) jacobian using funkyAD package \n",
    "    fx = f(x)\n",
    "    dfx = AD(f).grad(x) # alternative this can be written grad(f)(x)\n",
    "    x_next = x - fx/dfx \n",
    "    return x_next \n",
    "\n",
    "def get_root(f,x_start):\n",
    "    delta = 10\n",
    "    path = [x_start]\n",
    "    x = x_start\n",
    "    while delta > 1e-6:\n",
    "        new_x = newtonroot(f,x) \n",
    "        path.append(new_x)\n",
    "        delta = abs(x - new_x)\n",
    "        x = new_x \n",
    "    return(x, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root method for scalar function (e.g. y = f(x))\n",
    "def f(x):\n",
    "    return x**2+x \n",
    "\n",
    "# try 1 initialization\n",
    "zero1, path1 = get_root(f, 1)\n",
    "\n",
    "# try a different initialization \n",
    "zero2, path2  = get_root(f, -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(-2.5, 1.5, 100)\n",
    "path1_ys = [0]*len(path1)\n",
    "path2_ys = [0]*len(path2)\n",
    "\n",
    "\n",
    "plt.plot(xs, f(xs))\n",
    "plt.axhline(y=0, color='r', linestyle='-')\n",
    "plt.scatter(path1, path1_ys)\n",
    "plt.scatter(path2, path2_ys)\n",
    "plt.title('Root finding for x^2 + x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software Organization\n",
    "\n",
    "#### Directory Structure: \n",
    "```\n",
    ".\n",
    "| README.md\n",
    "| LICSENSE\n",
    "| requirements.txt\n",
    "| setup.py\n",
    "| \n",
    "|--docs\n",
    "|\n",
    "|  | milestone1.md\n",
    "|  | milestone2.ipynb\n",
    "|  |-examples\n",
    "|  | |- AD_users_intro.ipynb \n",
    "|\n",
    "|--funkyAD\n",
    "|  | __init__.py \n",
    "|  | base.py\n",
    "|  | functions.py \n",
    "|  | helpers.py\n",
    "|  |-tests\n",
    "|\n",
    "|--benchmarks\n",
    "```\n",
    "\n",
    "#### Modules (functionality)\n",
    "\n",
    "The basic package will include a module for forward AD, which takes a function as an argument and can return any of the following: the derivative, the derivative evaluated at a given value, the trace, etc. We also hope to implement a backward AD module, which takes the same input and returns the same output as the forward AD module, but uses backpropagation to calculate the return values and is more efficient for single-output functions.\n",
    "\n",
    "##### Feedback from milestone1 comments\n",
    "Module explanation is very high-level; more detail about each module's functionality would be nice. \n",
    "\n",
    "#### Test suite\n",
    "Our test suite will live within the funkyAD directory. We plan to use TravisCI, CodeCov, doctests and unittests for testing. We plan to use unittests for most of our testing (dottests only occasionally). We aim for as much code coverage as possible, and specifically target Exception raising and handling of edge cases.\n",
    "\n",
    "We plan to write extensive and detailed docstrings for all functions that will be accessible to the user, and replicate those in a nicer format in the corresponding docs folder (e.g. the file /funkyAD/AD.py will have its docs in /funkyAD/AD.html). A nice collection of examples for installation and usage will hopefully help the user in getting a working knowledge of the library quickly.\n",
    "\n",
    "#### Distribution\n",
    "The funkyAD package will be distributed with PyPI. Consequently, users will be able to install the package using the ubiquitous pip package manager. It will follow the guidelines and instructions in the official Python documentation. Installation instructions\n",
    "\n",
    "\tconda create -n env_name python=3.6 anaconda\n",
    "\tsource activate env_name\n",
    "\tpip install -r requirements.txt\n",
    "\tpip install -i https://test.pypi.org/simple/ funkyAD-funkyADers==0.0.3\n",
    "\n",
    "\n",
    "#### Software packaging \n",
    "According to common practice, we will include \\_\\_init\\_\\_.py files and setup.py so that automated tools can install and set up the library properly. We will choose transparent file names so that import statements are intuitive to the end user.\n",
    "\n",
    "It will be packaged as a Wheel for fast and easy installation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "#### Implementation (4/5)\n",
    "1. You needed a more in-depth explanation of how you will deal with vector-valued functions.\n",
    "2. How will you be storing \"eval_trace\" and \"graph\" in the AD class?\n",
    "\n",
    "In funkyAD we define 3 main classes: AD, Node, and ElementaryFunction. AD is the class that the user will interact with. It takes in an arbitrary function from the user and calls the necessary functions and classes in order to calculate the gradient. It will have transparently named methods and syntactic sugar when appropriate. The user should not know how anything else in the library works for simple usage.\n",
    "\n",
    "The Node class is essentially a row in our trace table, it has subclasses for input nodes (InputNode) and output nodes (OutputNodes). Nodes are connected and can be added or multiplied together to form new nodes, via the dunder methods\\_\\_add\\_\\_ etc, which allows us to build up the trace table. Dual numbers are essentially encoded as the pair (Node.val, Node.grad_val) in our implementation, and additions/operation are performed when calling ElementaryFunction(Node1, Node2). A Node is, essentially, an extension of the Box class in HW4. If the user has defined a function f, we will call f(InputNode(input)) and the successive operation performed on that node will allow us to recover the evaluation trace.\n",
    "\n",
    "The ElementaryFunction Class defines the functions and derivatives of elementary functions passed in by the user, such as sin, log, etc. We also allow the user to add their own elementary function to the list if we do not include the elementary function they need in the initial library list. An appropriate exception is raised if the user tries to utilize a function that is not defined as an instance of the ElementaryFunction class.\n",
    "\n",
    "```\n",
    "# External dependencies\n",
    "import numpy\n",
    "import doctest\n",
    "import unitest\n",
    "import pytest\n",
    "\n",
    "# the AD class instigates the differentation process and stores all output values\n",
    "class AD():\n",
    "\n",
    "Methods:\n",
    " \n",
    "def __init__(): initialize AD object \n",
    "def _buildgraph(): build the evaluation graph\n",
    "def gradient(forward): run forward AD, allows for option for backward AD if implemented\n",
    "def _createnodes(): create notes for the evaluation trace\n",
    "def set_seed(): set the seed (by default is the identity matrix)\n",
    "def get_seed(): return the seed\n",
    "def eval_trace: return evaluation trace  \n",
    "def print_graph(): return the evaluation graph   \n",
    "\n",
    "Attributes: \n",
    "\n",
    "nodeList: list of nodes\n",
    "inputNodeList: the list of inputs\n",
    "outputNodelist: the list of outputs\n",
    "eval_trace: the evaluation trace   \n",
    "graph: the evaluation graph \n",
    "history: history of calls memoized for increased performance\n",
    "(dunder methods e.g. __str__ redefined)\n",
    "\n",
    "# Node classes and subclasses \n",
    "class Node(): the node class stores relevant information for each node in the graph\n",
    "class InputNode(Node): a subclass of the Node class\n",
    "class OutputNode(Node): a subclass of the Node class\n",
    "\n",
    "Methods:\n",
    "__add__  (__radd__): add two nodes\n",
    "__mult__ (__rmult__): multiple two nodes\n",
    "(other dudner methods redefined as appropriate)\n",
    "\n",
    "Attributes:   \n",
    "val: the value of the node   \n",
    "gradient_val: the gradient of the node\n",
    "parents: list of parent nodes\n",
    "children: list of children nodes\n",
    "el: elementary function that the node computes (an ElementaryFunction object)\n",
    "SHOULD THIS BE STORED AS A DUAL NUMBER?\n",
    "\n",
    "# The Elementary Function stores elementary functions and their derivatives\n",
    "class ElementaryFunction(): \n",
    "\n",
    "Methods:\n",
    "__call__(): returns the output of the function\n",
    "derivative(): returns the derivative for a given input\n",
    "\n",
    "Attributes: \n",
    "ninputs: number of inputs  \n",
    "noutputs: number of outputs \n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
